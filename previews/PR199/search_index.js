var documenterSearchIndex = {"docs":
[{"location":"tutorials/generalized_likelihood/#Generalized-Likelihood-Inference","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"","category":"section"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"In this example we will demo the likelihood-based approach to parameter fitting. First let's generate a dataset to fit. We will re-use the Lotka-Volterra equation but in this case fit just two parameters.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"f1 = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3.0 * u[2] + u[1]*u[2]\nend\np = [1.5,1.0]\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(f1,u0,tspan,p)\nsol = solve(prob1,Tsit5())","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This is a function with two parameters, [1.5,1.0] which generates the same ODE solution as before. This time, let's generate 100 datasets where at each point adds a little bit of randomness:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0,stop=10,length=200))\nfunction generate_data(sol,t)\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(sol,t) for i in 1:100]))","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"here with t we measure the solution at 200 evenly spaced points. Thus aggregate_data is a 2x200x100 matrix where aggregate_data[i,j,k] is the ith component at time j of the kth dataset. What we first want to do is get a matrix of distributions where distributions[i,j] is the likelihood of component i at take j. We can do this via fit_mle on a chosen distributional form. For simplicity we choose the Normal distribution. aggregate_data[i,j,:] is the array of points at the given component and time, and thus we find the distribution parameters which fits best at each time point via:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Distributions\ndistributions = [fit_mle(Normal,aggregate_data[i,j,:]) for i in 1:2, j in 1:200]","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Notice for example that we have:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"julia> distributions[1,1]\nDistributions.Normal{Float64}(μ=1.0022440583676806, σ=0.009851964521952437)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"that is, it fit the distribution to have its mean just about where our original solution was and the variance is about how much noise we added to the dataset. This this is a good check to see that the distributions we are trying to fit our parameters to makes sense.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Note that in this case the Normal distribution was a good choice, and in many cases it's a nice go-to choice, but one should experiment with other choices of distributions as well. For example, a TDist can be an interesting way to incorporate robustness to outliers since low degrees of free T-distributions act like Normal distributions but with longer tails (though fit_mle does not work with a T-distribution, you can get the means/variances and build appropriate distribution objects yourself).","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Once we have the matrix of distributions, we can build the objective function corresponding to that distribution fit:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using DiffEqParamEstim\nobj = build_loss_objective(prob1,Tsit5(),LogLikeLoss(t,distributions),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"First let's use the objective function to plot the likelihood landscape:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Plots; plotly()\nprange = 0.5:0.1:5.0\nheatmap(prange,prange,[obj([j,i]) for i in prange, j in prange],\n        yscale=:log10,xlabel=\"Parameter 1\",ylabel=\"Parameter 2\",\n        title=\"Likelihood Landscape\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 2 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Recall that this is the negative loglikelihood and thus the minimum is the maximum of the likelihood. There is a clear valley where the first parameter is 1.5, while the second parameter's likelihood is more muddled. By taking a one-dimensional slice:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"plot(prange,[obj([1.5,i]) for i in prange],lw=3,\n     title=\"Parameter 2 Likelihood (Parameter 1 = 1.5)\",\n     xlabel = \"Parameter 2\", ylabel = \"Objective Function Value\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"we can see that there's still a clear minimum at the true value. Thus we will use the global optimizers from BlackBoxOptim.jl to find the values. We set our search range to be from 0.5 to 5.0 for both of the parameters and let it optimize:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using BlackBoxOptim\nbound1 = Tuple{Float64, Float64}[(0.5, 5),(0.5, 5)]\nresult = bboptimize(obj;SearchRange = bound1, MaxSteps = 11e3)\n\nStarting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},B\nlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound\n{BlackBoxOptim.RangePerDimSearchSpace}}\n0.00 secs, 0 evals, 0 steps\n0.50 secs, 1972 evals, 1865 steps, improv/step: 0.266 (last = 0.2665), fitness=-737.311433781\n1.00 secs, 3859 evals, 3753 steps, improv/step: 0.279 (last = 0.2913), fitness=-739.658421879\n1.50 secs, 5904 evals, 5799 steps, improv/step: 0.280 (last = 0.2830), fitness=-739.658433715\n2.00 secs, 7916 evals, 7811 steps, improv/step: 0.225 (last = 0.0646), fitness=-739.658433715\n2.50 secs, 9966 evals, 9861 steps, improv/step: 0.183 (last = 0.0220), fitness=-739.658433715\n\nOptimization stopped after 11001 steps and 2.7839999198913574 seconds\nTermination reason: Max number of steps (11000) reached\nSteps per second = 3951.50873439296\nFunction evals per second = 3989.2242527195904\nImprovements/step = 0.165\nTotal function evaluations = 11106\n\n\nBest candidate found: [1.50001, 1.00001]\n\nFitness: -739.658433715","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This shows that it found the true parameters as the best fit to the likelihood.","category":"page"},{"location":"methods/collocation_loss/#Two-Stage-method-(Non-Parametric-Collocation)","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"","category":"section"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"The two-stage method is a collocation method for estimating parameters without requiring repeated solving of the differential equation. It does so by determining a smoothed estimated trajectory of the data (local quadratic polynomial fit by least squares) and optimizing the derivative function and the data's timepoints to match the derivatives of the smoothed trajectory. This method has less accuracy than other methods but is much faster, and is a good method to try first to get in the general \"good parameter\" region, to then finish using one of the other methods.","category":"page"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"function two_stage_objective(prob::DEProblem, tpoints, data, adtype = SciMLBase.NoAD(),;\n                          kernel= :Epanechnikov,\n                          loss_func = L2DistLoss)","category":"page"},{"location":"tutorials/global_optimization/#Global-Optimization-via-NLopt","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"","category":"section"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"The build_loss_objective function builds an objective function which is able to be used with MathOptInterface-associated solvers. This includes packages like IPOPT, NLopt, MOSEK, etc. Building off of the previous example, we can build a cost function for the single parameter optimization problem like:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"function f(du,u,p,t)\n  dx = p[1]*u[1] - u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())\n\nt = collect(range(0,stop=10,length=200))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)\n\nobj = build_loss_objective(prob,Tsit5(),L2Loss(t,data),maxiters=10000)","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can now use this obj as the objective function with MathProgBase solvers. For our example, we will use NLopt. To use the local derivative-free Constrained Optimization BY Linear Approximations algorithm, we can simply do:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_COBYLA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"This finds a minimum at [1.49997]. For a modified evolutionary algorithm, we can use:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ESCH, 1)\nmin_objective!(opt, obj)\nlower_bounds!(opt,[0.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can even use things like the Improved Stochastic Ranking Evolution Strategy (and add constraints if needed). This is done via:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ISRES, 1)\nmin_objective!(opt, obj.cost_function2)\nlower_bounds!(opt,[-1.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[0.2])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"which is very robust to the initial condition. The fastest result comes from the following:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_BOBYQA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"For more information, see the NLopt documentation for more details. And give IPOPT or MOSEK a try!","category":"page"},{"location":"tutorials/jump/#Using-JuMP-with-DiffEqParamEstim","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"","category":"section"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"JuMP is a domain-specific modeling language for mathematical optimization embedded in Julia.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"using OrdinaryDiffEq, DiffEqParamEstim, JuMP, NLopt, Plots","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's define the Lorenz equation to use as our example","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"function g(du,u,p,t)\n  σ,ρ,β = p\n  x,y,z = u\n  du[1] = dx = σ*(y-x)\n  du[2] = dy = x*(ρ-z) - y\n  du[3] = dz = x*y - β*z\nend","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's get a solution of the system with parameter values σ=10.0 ρ=28.0 β=8/3 to use as our data. We define some convenience functions model_ode (to create an ODEProblem) and solve_model(to obtain solution of the ODEProblem) to use in a custom objective function later.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"u0 = [1.0;0.0;0.0]\nt = 0.0:0.01:1.0\ntspan = (0.0,1.0)\nmodel_ode(p_) = ODEProblem(g, u0, tspan,p_)\nsolve_model(mp_) = OrdinaryDiffEq.solve(model_ode(mp_), Tsit5(),saveat=0.01)\nmock_data = Array(solve_model([10.0,28.0,8/3]))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Now we define a custom objective function to pass for optimization to JuMP using the build_loss_objective described above provided by DiffEqParamEstim that defines an objective function for the parameter estimation problem.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"loss_objective(mp_, dat) = build_loss_objective(model_ode(mp_), Tsit5(), L2Loss(t,dat))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"We create a JuMP model, variables, set the objective function and the choice of optimization algorithm to be used in the JuMP syntax. You can read more about this in JuMP's documentation.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"juobj(args...) = loss_objective(args, mock_data)(args)\njumodel = Model()\nJuMP.register(jumodel, :juobj, 3, juobj, autodiff=true)\n@variables jumodel begin\n    σ,(start=8)\n    ρ,(start=25.0)\n    β,(start=10/3)\nend\n@NLobjective(jumodel, Min, juobj(σ, ρ, β))\nsetsolver(jumodel, NLoptSolver(algorithm=:LD_MMA))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's call the optimizer to obtain the fitted parameter values.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = JuMP.solve(jumodel)\nbest_mp = getvalue.(getindex.((jumodel,), Symbol.(jumodel.colNames)))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's compare the solution at the obtained parameter values and our data.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = OrdinaryDiffEq.solve(best_mp |> model_ode, Tsit5())\nplot(getindex.(sol.(t),1))\nscatter!(mock_data, markersize=2)","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"(Image: jumpestimationplot)","category":"page"},{"location":"tutorials/stochastic_evaluations/#Parameter-Estimation-for-Stochastic-Differential-Equations-and-Ensembles","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"","category":"section"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We can use any DEProblem, which not only includes DAEProblem and DDEProblems, but also stochastic problems. In this case, let's use the generalized maximum likelihood to fit the parameters of an SDE's ensemble evaluation.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Let's use the same Lotka-Volterra equation as before, but this time add noise:","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"pf_func = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0]\npg_func = function (du,u,p,t)\n  du[1] = 1e-6u[1]\n  du[2] = 1e-6u[2]\nend\nprob = SDEProblem(pf_func,pg_func,u0,tspan,p)\nsol = solve(prob,SRIW1())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now lets generate a dataset from 10,000 solutions of the SDE","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0, stop=10, length=200))\nfunction generate_data(t)\n  sol = solve(prob,SRIW1())\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(t) for i in 1:10000]))","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now let's estimate the parameters. Instead of using single runs from the SDE, we will use a EnsembleProblem. This means that it will solve the SDE N times to come up with an approximate probability distribution at each time point and use that in the likelihood estimate.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"monte_prob = EnsembleProblem(prob)","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We use Optim.jl for optimization below","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"obj = build_loss_objective(monte_prob,SOSRI(),L2Loss(t,aggregate_data),\n                                     maxiters=10000,verbose=false,num_monte = 1000,\n                                     parallel_type = :threads)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Parameter Estimation in case of SDE's with a regular L2Loss can have poor accuracy due to only fitting against the mean properties as mentioned in First Differencing.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Results of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [6.070728870478734,5.113357737345448]\n * Minimum: 1.700440e+03\n * Iterations: 14\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 1.00e-03\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.81e-07 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 2.34e+00\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 61\n * Gradient Calls: 61","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Instead when we use L2Loss with first differencing enabled we get much more accurate estimates.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":" obj = build_loss_objective(monte_prob,SRIW1(),L2Loss(t,data,differ_weight=1.0,data_weight=0.5),maxiters=1000,\n                                  verbose=false,verbose_opt=false,verbose_steps=1,num_monte=50)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())\nResults of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [1.5010687426045128,1.0023453619050238]\n * Minimum: 1.166650e-01\n * Iterations: 16\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 6.84e-09\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.85e-06 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 1.81e-01\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 118\n * Gradient Calls: 118","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Here, we see that we successfully recovered the drift parameter, and got close to the original noise parameter after searching a two-orders-of-magnitude range.","category":"page"},{"location":"methods/recommended_methods/#Recommended-Methods","page":"Recommended Methods","title":"Recommended Methods","text":"","category":"section"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"The recommended method is to use build_loss_objective with the optimizer of your choice. This method can thus be paired with global optimizers from packages like BlackBoxOptim.jl or NLopt.jl which can be much less prone to finding local minima than local optimization methods. Also, it allows the user to define the cost function in the way they choose as a function loss(sol), and thus can fit using any cost function on the solution, making it applicable to fitting non-temporal data and other types of problems. Also, build_loss_objective works for all of the DEProblem types, allowing it to optimize parameters on ODEs, SDEs, DDEs, DAEs, etc.","category":"page"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"However, this method requires repeated solution of the differential equation. If the data is temporal data, the most efficient method is the two_stage_objective which does not require repeated solutions but is not as accurate. Usage of the two_stage_objective should have a post-processing step which refines using a method like build_loss_objective.","category":"page"},{"location":"methods/optimization_based_methods/#Optimization-Based-Methods","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Objective-Function-Builders","page":"Optimization-Based Methods","title":"The Objective Function Builders","text":"","category":"section"},{"location":"methods/optimization_based_methods/#Standard-Nonlinear-Regression","page":"Optimization-Based Methods","title":"Standard Nonlinear Regression","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"build_loss_objective builds an objective function to be used with Optim.jl and MathProgBase-associated solvers like NLopt.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function build_loss_objective(prob::DEProblem, alg, loss,\n                              adtype = SciMLBase.NoAD(),\n                              regularization = nothing;\n                              priors = nothing,\n                              prob_generator = STANDARD_PROB_GENERATOR,\n                              kwargs...)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The first argument is the DEProblem to solve, and next is the alg to use. The alg must match the problem type, which can be any DEProblem (ODEs, SDEs, DAEs, DDEs, etc.). regularization defaults to nothing which has no regularization function. The extra keyword arguments are passed to the differential equation solver.","category":"page"},{"location":"methods/optimization_based_methods/#Multiple-Shooting","page":"Optimization-Based Methods","title":"Multiple Shooting","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Multiple Shooting is generally used in Boundary Value Problems (BVP) and is more robust than the regular objective function used in these problems. It proceeds as follows:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Divide up the time span into short time periods and solve the equation with the current parameters which here consist of both, the parameters of the differential equations and also the initial values for the short time periods.\nThis objective additionally involves a discontinuity error term that imposes higher cost if the end of the solution of one time period doesn't match the beginning of the next one.\nMerge the solutions from the shorter intervals and then calculate the loss.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function multiple_shooting_objective(prob::DiffEqBase.DEProblem, alg, loss,\n                              adtype = SciMLBase.NoAD(),\n                              regularization = nothing;\n                              priors = nothing,\n                              discontinuity_weight = 1.0,\n                              prob_generator = STANDARD_PROB_GENERATOR,\n                              kwargs...)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For consistency multiple_shooting_objective takes exactly the same arguments as build_loss_objective. It also has the option for discontinuity_weight as a keyword argument which assigns weight to the error occurring due to the discontinuity that arises from the breaking up of the time span.","category":"page"},{"location":"methods/optimization_based_methods/#Detailed-Explanations-of-Arguments","page":"Optimization-Based Methods","title":"Detailed Explanations of Arguments","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Loss-Function","page":"Optimization-Based Methods","title":"The Loss Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"loss(sol)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"is the function which reduces the problem's solution to a scalar which the optimizer will try to minimize. While this is very flexible, two convenience routines are included for fitting to data with standard cost functions:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t, data; differ_weight=nothing, data_weight=nothing,\n              colloc_grad=nothing, dudt=nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"where t is the set of timepoints which the data is found at, and data are the values that are known where each column corresponds to measures of the values of the system. L2Loss is an optimized version of the L2-distance. The data_weight is a scalar or vector of weights for the loss function which must match the size of the data. Note that minimization of a weighted L2Loss is equivalent to maximum likelihood estimation of a heteroskedastic Normally distributed likelihood. differ_weight allows one to add a weight on the first differencing terms sol[i+1]-sol[i] against the data first differences. This smooths out the loss term and can make it easier to fit strong solutions of stochastic models, but is zero (nothing) by default. Additionally, colloc_grad allows one to give a matrix of the collocation gradients for the data. This is used to add an interpolation derivative term, like the two-stage method. A convenience function colloc_grad(t,data) returns a collocation gradient from a 3rd order spline calculated by Dierckx.jl, which can be used as the colloc_grad. Note that, with a collocation gradient and regularization, this loss is equivalent to a 4DVAR.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Additionally, we include a more flexible log-likelihood approach:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"LogLikeLoss(t, distributions, diff_distributions=nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"In this case, there are two forms. The simple case is where distributions[i,j] is the likelihood distributions from a UnivariateDistribution from Distributions.jl, where it corresponds to the likelihood at t[i] for component j. The second case is where distributions[i] is a MultivariateDistribution which corresponds to the likelihood at t[i] over the vector of components. This likelihood function then calculates the negative of the total loglikelihood over time as its objective value (negative since optimizers generally find minimums, and thus this corresponds to maximum likelihood estimation). The third term, diff_distributions, acts similarly but allows putting a distribution on the first difference terms sol[i+1]-sol[i].","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Note that these distributions can be generated via fit_mle on some dataset against some chosen distribution type.","category":"page"},{"location":"methods/optimization_based_methods/#Note-About-Loss-Functions","page":"Optimization-Based Methods","title":"Note About Loss Functions","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For parameter estimation problems, it's not uncommon for the optimizers to hit unstable regions of parameter space. This causes warnings that the solver exited early, and the built-in loss functions like L2Loss automatically handle this. However, if using a user-supplied loss function, you should make sure it's robust to these issues. One common pattern is to apply infinite loss when the integration is not successful. Using the retcodes, this can be done via:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function my_loss_function(sol)\n   tot_loss = 0.0\n   if any((s.retcode != :Success for s in sol))\n     tot_loss = Inf\n   else\n     # calculation for the loss here\n   end\n   tot_loss\nend","category":"page"},{"location":"methods/optimization_based_methods/#Note-on-First-Differencing","page":"Optimization-Based Methods","title":"Note on First Differencing","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t, data, differ_weight=0.3, data_weight=0.7)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"First differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the L2Loss alone leads to non-identifiable parameters but adding a first differencing term makes it more identifiable. This can be noted on stochastic differential equation models, where this aims to capture the autocorrelation and therefore helps us avoid getting the same stationary distribution despite different trajectories and thus wrong parameter estimates.","category":"page"},{"location":"methods/optimization_based_methods/#The-Regularization-Function","page":"Optimization-Based Methods","title":"The Regularization Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization can be any function of p, the parameter vector:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"regularization(p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The Regularization helper function builds a regularization using a penalty function penalty from PenaltyFunctions.jl:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"reg = Regularization(λ, penalty=L2Penalty())\nbuild_loss_objective(prob, alg, loss, SciMLBase.NoAD(), reg)\n\nusing Optimization, Zygote\nbuild_loss_objective(prob, alg, loss, Optimization.AutoZygote(), reg)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization defaults to L2 if no penalty function is specified. λ is the weight parameter for the addition of the regularization term.","category":"page"},{"location":"methods/optimization_based_methods/#Using-automatic-differentiation","page":"Optimization-Based Methods","title":"Using automatic differentiation","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"To use derivatives with optimization solvers, Optimization.jl's adtype argument as described here should be used with the wrapper subpackage OptimizationOptimJL, OptimizationNLopt etc.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"using Optimization, ForwardDiff\nbuild_loss_objective(prob, alg, loss, Optimization.AutoForwardDiff())\nmultiple_shooting_objective(prob, alg, loss, Optimization.AutoForwardDiff())","category":"page"},{"location":"methods/optimization_based_methods/#The-Problem-Generator-Function","page":"Optimization-Based Methods","title":"The Problem Generator Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The argument prob_generator allows one to specify a function for generating new problems from a given parameter set. By default, this just builds a new problem which fixes the element types in a way that's autodifferentiation compatible and adds the new parameter vector p. For example, the code for this is:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob, u0=convert.(eltype(p), prob.u0), p=p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Then the new problem with these new values is returned.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"One can use this to change the meaning of the parameters using this function. For example, if one instead wanted to optimize the initial conditions for a function without parameters, you could change this to:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob.f, u0=p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"which simply uses p as the initial condition in the initial value problem.","category":"page"},{"location":"methods/optimization_based_methods/#Using-the-Objectives-for-MAP-estimates","page":"Optimization-Based Methods","title":"Using the Objectives for MAP estimates","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"You can also add a prior option to build_loss_objective and multiple_shooting_objective that essentially turns it into MAP by multiplying the loglikelihood (the cost) by the prior. The option is available as the keyword argument priors, it can take in either an array of univariate distributions for each of the parameters or a multivariate distribution.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"ms_obj = multiple_shooting_objective(ms_prob, Tsit5(), L2Loss(t,data); priors = priors, discontinuity_weight = 1.0, abstol = 1e-12, reltol = 1e-12)","category":"page"},{"location":"#DiffEqParamEstim.jl:-Parameter-Estimation-for-Differential-Equations","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"DiffEqParamEstim.jl is a package for simplified parameter estimation with DifferentialEquations.jl While not as expansive as SciMLSensitivity.jl, it's provides a simple interface for users who want to quickly run standard parameter estimation routines for model calibration on not too large of models (<100 parameters or ODEs). It is designed to integrate with Optimization.jl interface or directly use with an optimization package.","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"note: Note\nFor much larger models and more complex setups (multiple datasets, batching, etc.) see SciMLSensitivity.","category":"page"},{"location":"#Installation","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"To install DiffEqParamEstim.jl, use the Julia package manager:","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg\nPkg.add(\"DiffEqParamEstim\")","category":"page"},{"location":"#Contributing","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Contributing","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"You can also download the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"\">project</a> file.","category":"page"},{"location":"tutorials/ODE_inference/#Optimization-Based-ODE-Parameter-Estimation","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"","category":"section"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We choose to optimize the parameters on the Lotka-Volterra equation. We do so by defining the function as a function with parameters:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using DifferentialEquations, RecursiveArrayTools, Plots, Optim, DiffEqParamEstim, BlackBoxOptim\nusing Optimization, Zygote, OptimizationOptimJL, OptimizationBBO, SciMLSensitivity\n\nfunction f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - u[1]*u[2]\n  du[2] = dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We create data using the numerical result with a=1.5:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"sol = solve(prob,Tsit5())\nt = collect(range(0,stop=10,length=200))\nusing RecursiveArrayTools # for VectorOfArray\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we used VectorOfArray from RecursiveArrayTools.jl to turn the result of an ODE into a matrix.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"If we plot the solution with the parameter at a=1.42, we get the following:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Not Fit)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Notice that after one period this solution begins to drift very far off: this problem is sensitive to the choice of a.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"To build the objective function for Optim.jl, we simply call the build_loss_objective function:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set maxiters in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set verbose=false because this divergence can get noisy.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Before optimizing, let's visualize our cost function by plotting it for a range of parameter values: yscale","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"vals = 0.0:0.1:10.0\nplot(vals,[cost_function(i) for i in vals],yscale=:log10,\n     xaxis = \"Parameter\", yaxis = \"Cost\", title = \"1-Parameter Cost Function\",\n     lw = 3)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Now this cost function can be used with Optim.jl in order to get the parameters. For example, we can use Brent's algorithm to search for the best solution on the interval [0,10] by:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using Optim\nresult = optimize(cost_function, 0.0, 10.0)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This returns result.minimizer[1]==1.5 as the best parameter to match the data. When we plot the fitted equation on the data, we receive the following:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Fit)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Thus we see that after fitting, the lines match up with the generated data and receive the right parameter value.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use the multivariate optimization functions. For example, we can use the BFGS algorithm to optimize the parameter starting at a=1.42. By default, Optim.jl only uses ForwardDiff to generate the derivatives, but with the Optimization.jl interface that decouples the derivative generation from the optimization library we can use any library we want. Here we use Zygote.jl to generate the derivatives:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob, Tsit5(), L2Loss(t,data), Optimization.AutoZygote(),\n                                      maxiters=10000,verbose=false)\noptprob = Optimization.OptimizationProblem(cost_function, [1.42])\nresult = solve(optprob, BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Note that some of the algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the documentation for Optim.jl. We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus following the Optim.jl documentation we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm. We pass the lb and ub keyword arguments to the OptimizationProblem to pass these bounds to the optimizer:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"lower = [0.0]\nupper = [3.0]\noptprob = Optimization.OptimizationProblem(cost_function, [1.42], lb = lower, ub = upper)\nresult = solve(optprob, BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let's use the Lotka-Volterra equation with all parameters free:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function f2(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f2,u0,tspan,p)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can build an objective function and solve the multiple parameter version just as before:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"optprob = Optimization.OptimizationProblem(cost_function, [1.3,0.8,2.8,1.2])\nresult_bfgs = solve(optprob, BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use First-Differences in L2Loss by passing the kwarg differ_weight which decides the contribution of the differencing loss to the total loss.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data,differ_weight=0.3,data_weight=0.7), Optimization.AutoZygote(),\n                                      maxiters=10000,verbose=false)\noptprob = OptimizationProblem(cost_function, [1.3,0.8,2.8,1.2])\nresult_bfgs = solve(optprob, BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use Multiple Shooting method by creating a multiple_shooting_objective","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function ms_f(du,u,p,t)\n  dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\nms_u0 = [1.0;1.0]\ntspan = (0.0,10.0)\nms_p = [1.5,1.0]\nms_prob = ODEProblem(ms_f,ms_u0,tspan,ms_p)\nt = collect(range(0,stop=10,length=200))\ndata = Array(solve(ms_prob,Tsit5(),saveat=t,abstol=1e-12,reltol=1e-12))\nbound = Tuple{Float64, Float64}[(0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10)]\n\n\nms_obj = multiple_shooting_objective(ms_prob, Tsit5(), L2Loss(t, data), Optimization.AutoZygote(); discontinuity_weight = 1.0, abstol = 1e-12, reltol = 1e-12)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This creates the objective function that can be passed to an optimizer from which we can then get the parameter values and the initial values of the short time periods keeping in mind the indexing.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"result = bboptimize(ms_obj; SearchRange = bound, MaxSteps = 21e3)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"result.archive_output.best_candidate[end-1:end]","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here as our model had 2 parameters, we look at the last two indexes of result to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The objective function for Two Stage method can be created and passed to an optimizer as","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"two_stage_obj = two_stage_objective(ms_prob, t, data)\noptprob = Optimization.OptimizationProblem(two_stage_obj, [1.3,0.8,2.8,1.2])\nresult = solve(optprob, Optim.BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The default kernel used in the method is Epanechnikov others that are available are Uniform,  Triangular, Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic and Sigmoid, this can be passed by the kernel keyword argument. loss_func keyword argument can be used to pass the loss function (cost function) you want  to use and passing a valid adtype argument enables Auto Differentiation.","category":"page"},{"location":"tutorials/ensemble/#Fitting-Ensembles-of-ODE-Models-to-Data","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"","category":"section"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"In this tutoiral we will showcase how to fit multiple models simultaniously to respective data sources. Let's dive right in!","category":"page"},{"location":"tutorials/ensemble/#Formulating-the-Ensemble-Model","page":"Fitting Ensembles of ODE Models to Data","title":"Formulating the Ensemble Model","text":"","category":"section"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"First you want to create a problem which solves multiple problems at the same time. This is the EnsembleProblem. When the parameter estimation tools say it will take any DEProblem, it really means ANY DEProblem, which includes EnsembleProblem.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"So, let's get an EnsembleProblem setup that solves with 10 different initial conditions. This looks as follows:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"using DifferentialEquations, DiffEqParamEstim, Plots, Optimization, ForwardDiff, OptimizationOptimJL\n\n# Monte Carlo Problem Set Up for solving set of ODEs with different initial conditions\n\n# Set up Lotka-Volterra system\nfunction pf_func(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\np = [1.5,1.0]\nprob = ODEProblem(pf_func,[1.0,1.0],(0.0,10.0),p)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now for an EnsembleProblem we have to take this problem and tell it what to do N times via the prob_func. So let's generate N=10 different initial conditions, and tell it to run the same problem but with these 10 different initial conditions each time:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Setting up to solve the problem N times (for the N different initial conditions)\nN = 10;\ninitial_conditions = [[1.0,1.0], [1.0,1.5], [1.5,1.0], [1.5,1.5], [0.5,1.0], [1.0,0.5], [0.5,0.5], [2.0,1.0], [1.0,2.0], [2.0,2.0]]\nfunction prob_func(prob,i,repeat)\n  ODEProblem(prob.f,initial_conditions[i],prob.tspan,prob.p)\nend\nenprob = EnsembleProblem(prob,prob_func=prob_func)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"We can check this does what we want by solving it:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Check above does what we want\nsim = solve(enprob,Tsit5(),trajectories=N)\nplot(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"trajectories=N means \"run N times\", and each time it runs the problem returned by the prob_func, which is always the same problem but with the ith initial condition.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now let's generate a dataset from that. Let's get data points at every t=0.1 using saveat, and then convert the solution into an array.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Generate a dataset from these runs\ndata_times = 0.0:0.1:10.0\nsim = solve(enprob,Tsit5(),trajectories=N,saveat=data_times)\ndata = Array(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Here, data[i,j,k] is the same as sim[i,j,k] which is the same as sim[k][i,j] (where sim[k] is the kth solution). So data[i,j,k] is the jth timepoint of the ith variable in the kth trajectory.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now let's build a loss function. A loss function is some loss(sol) that spits out a scalar for how far from optimal we are. In the documentation I show that we normally do loss = L2Loss(t,data), but we can bootstrap off of this. Instead lets build an array of N loss functions, each one with the correct piece of data.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Building a loss function\nlosses = [L2Loss(data_times,data[:,:,i]) for i in 1:N]","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"So losses[i] is a function which computes the loss of a solution against the data of the ith trajectory. So to build our true loss function, we sum the losses:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"loss(sim) = sum(losses[i](sim[i]) for i in 1:N)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"As a double check, make sure that loss(sim) outputs zero (since we generated the data from sim). Now we generate data with other parameters:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"prob = ODEProblem(pf_func,[1.0,1.0],(0.0,10.0),[1.2,0.8])\nfunction prob_func(prob,i,repeat)\n  ODEProblem(prob.f,initial_conditions[i],prob.tspan,prob.p)\nend\nenprob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(enprob,Tsit5(),trajectories=N,saveat=data_times)\nloss(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"and get a non-zero loss. So we now have our problem, our data, and our loss function... we have what we need.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Put this into buildlossobjective.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"obj = build_loss_objective(enprob,Tsit5(),loss,Optimization.AutoForwardDiff(),trajectories=N,\n                           saveat=data_times)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Notice that we added the kwargs for solve of the EnsembleProblem into this. They get passed to the internal solve command, so then the loss is computed on N trajectories at data_times.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Thus we take this objective function over to any optimization package. Here, since the Lotka-Volterra equation requires positive parameters, we use Fminbox to make sure the parameters stay within passed bounds. Let's start the optimization with [1.3,0.9], Optim spits out that the true parameters are:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"lower = zeros(2)\nupper = fill(2.0,2)\noptprob = OptimizationProblem(obj,[1.3,0.9],lb = lower,ub = upper)\nresult = solve(optprob, Fminbox(BFGS()))","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"result","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Optim finds one but not the other parameter.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"It is advised to run a test on synthetic data for your problem before using it on real data. Maybe play around with different optimization packages, or add regularization. You may also want to decrease the tolerance of the ODE solvers via","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"obj = build_loss_objective(enprob,Tsit5(),loss, Optimization.AutoForwardDiff(), trajectories=N,\n                           abstol=1e-8,reltol=1e-8,\n                           saveat=data_times)\noptprob = OptimizationProblem(obj, [1.3,0.9], lb = lower, ub = upper)\nresult = solve(optprob, BFGS()) #OptimizationOptimJL detects that it's a box constrained problem and use Fminbox wrapper over BFGS","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"result","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"if you suspect error is the problem. However, if you're having problems it's most likely not the ODE solver tolerance and mostly because parameter inference is a very hard optimization problem.","category":"page"}]
}
