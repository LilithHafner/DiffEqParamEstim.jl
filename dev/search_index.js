var documenterSearchIndex = {"docs":
[{"location":"tutorials/generalized_likelihood/#Generalized-Likelihood-Inference","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"","category":"section"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"In this example we will demo the likelihood-based approach to parameter fitting. First let's generate a dataset to fit. We will re-use the Lotka-Volterra equation but in this case fit just two parameters.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"f1 = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3.0 * u[2] + u[1]*u[2]\nend\np = [1.5,1.0]\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(f1,u0,tspan,p)\nsol = solve(prob1,Tsit5())","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This is a function with two parameters, [1.5,1.0] which generates the same ODE solution as before. This time, let's generate 100 datasets where at each point adds a little bit of randomness:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0,stop=10,length=200))\nfunction generate_data(sol,t)\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(sol,t) for i in 1:100]))","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"here with t we measure the solution at 200 evenly spaced points. Thus aggregate_data is a 2x200x100 matrix where aggregate_data[i,j,k] is the ith component at time j of the kth dataset. What we first want to do is get a matrix of distributions where distributions[i,j] is the likelihood of component i at take j. We can do this via fit_mle on a chosen distributional form. For simplicity we choose the Normal distribution. aggregate_data[i,j,:] is the array of points at the given component and time, and thus we find the distribution parameters which fits best at each time point via:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Distributions\ndistributions = [fit_mle(Normal,aggregate_data[i,j,:]) for i in 1:2, j in 1:200]","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Notice for example that we have:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"julia> distributions[1,1]\nDistributions.Normal{Float64}(μ=1.0022440583676806, σ=0.009851964521952437)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"that is, it fit the distribution to have its mean just about where our original solution was and the variance is about how much noise we added to the dataset. This this is a good check to see that the distributions we are trying to fit our parameters to makes sense.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Note that in this case the Normal distribution was a good choice, and in many cases it's a nice go-to choice, but one should experiment with other choices of distributions as well. For example, a TDist can be an interesting way to incorporate robustness to outliers since low degrees of free T-distributions act like Normal distributions but with longer tails (though fit_mle does not work with a T-distribution, you can get the means/variances and build appropriate distribution objects yourself).","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Once we have the matrix of distributions, we can build the objective function corresponding to that distribution fit:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using DiffEqParamEstim\nobj = build_loss_objective(prob1,Tsit5(),LogLikeLoss(t,distributions),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"First let's use the objective function to plot the likelihood landscape:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Plots; plotly()\nprange = 0.5:0.1:5.0\nheatmap(prange,prange,[obj([j,i]) for i in prange, j in prange],\n        yscale=:log10,xlabel=\"Parameter 1\",ylabel=\"Parameter 2\",\n        title=\"Likelihood Landscape\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 2 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Recall that this is the negative loglikelihood and thus the minimum is the maximum of the likelihood. There is a clear valley where the first parameter is 1.5, while the second parameter's likelihood is more muddled. By taking a one-dimensional slice:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"plot(prange,[obj([1.5,i]) for i in prange],lw=3,\n     title=\"Parameter 2 Likelihood (Parameter 1 = 1.5)\",\n     xlabel = \"Parameter 2\", ylabel = \"Objective Function Value\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"we can see that there's still a clear minimum at the true value. Thus we will use the global optimizers from BlackBoxOptim.jl to find the values. We set our search range to be from 0.5 to 5.0 for both of the parameters and let it optimize:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using BlackBoxOptim\nbound1 = Tuple{Float64, Float64}[(0.5, 5),(0.5, 5)]\nresult = bboptimize(obj;SearchRange = bound1, MaxSteps = 11e3)\n\nStarting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},B\nlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound\n{BlackBoxOptim.RangePerDimSearchSpace}}\n0.00 secs, 0 evals, 0 steps\n0.50 secs, 1972 evals, 1865 steps, improv/step: 0.266 (last = 0.2665), fitness=-737.311433781\n1.00 secs, 3859 evals, 3753 steps, improv/step: 0.279 (last = 0.2913), fitness=-739.658421879\n1.50 secs, 5904 evals, 5799 steps, improv/step: 0.280 (last = 0.2830), fitness=-739.658433715\n2.00 secs, 7916 evals, 7811 steps, improv/step: 0.225 (last = 0.0646), fitness=-739.658433715\n2.50 secs, 9966 evals, 9861 steps, improv/step: 0.183 (last = 0.0220), fitness=-739.658433715\n\nOptimization stopped after 11001 steps and 2.7839999198913574 seconds\nTermination reason: Max number of steps (11000) reached\nSteps per second = 3951.50873439296\nFunction evals per second = 3989.2242527195904\nImprovements/step = 0.165\nTotal function evaluations = 11106\n\n\nBest candidate found: [1.50001, 1.00001]\n\nFitness: -739.658433715","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This shows that it found the true parameters as the best fit to the likelihood.","category":"page"},{"location":"methods/collocation_loss/#Two-Stage-method-(Non-Parametric-Collocation)","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"","category":"section"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"The two-stage method is a collocation method for estimating parameters without requiring repeated solving of the differential equation. It does so by determining a smoothed estimated trajectory of the data (local quadratic polynomial fit by least squares) and optimizing the derivative function and the data's timepoints to match the derivatives of the smoothed trajectory. This method has less accuracy than other methods but is much faster, and is a good method to try first to get in the general \"good parameter\" region, to then finish using one of the other methods.","category":"page"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"function two_stage_method(prob::DEProblem,tpoints,data;kernel= :Epanechnikov,\n                          loss_func = L2DistLoss,mpg_autodiff = false,\n                          verbose = false,verbose_steps = 100)","category":"page"},{"location":"tutorials/global_optimization/#Global-Optimization-via-NLopt","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"","category":"section"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"The build_loss_objective function builds an objective function which is able to be used with MathOptInterface-associated solvers. This includes packages like IPOPT, NLopt, MOSEK, etc. Building off of the previous example, we can build a cost function for the single parameter optimization problem like:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"function f(du,u,p,t)\n  dx = p[1]*u[1] - u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())\n\nt = collect(range(0,stop=10,length=200))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)\n\nobj = build_loss_objective(prob,Tsit5(),L2Loss(t,data),maxiters=10000)","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can now use this obj as the objective function with MathProgBase solvers. For our example, we will use NLopt. To use the local derivative-free Constrained Optimization BY Linear Approximations algorithm, we can simply do:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_COBYLA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"This finds a minimum at [1.49997]. For a modified evolutionary algorithm, we can use:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ESCH, 1)\nmin_objective!(opt, obj)\nlower_bounds!(opt,[0.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can even use things like the Improved Stochastic Ranking Evolution Strategy (and add constraints if needed). This is done via:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ISRES, 1)\nmin_objective!(opt, obj.cost_function2)\nlower_bounds!(opt,[-1.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[0.2])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"which is very robust to the initial condition. The fastest result comes from the following:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_BOBYQA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"For more information, see the NLopt documentation for more details. And give IPOPT or MOSEK a try!","category":"page"},{"location":"tutorials/jump/#Using-JuMP-with-DiffEqParamEstim","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"","category":"section"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"JuMP is a domain-specific modeling language for mathematical optimization embedded in Julia.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"using OrdinaryDiffEq, DiffEqParamEstim, JuMP, NLopt, Plots","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's define the Lorenz equation to use as our example","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"function g(du,u,p,t)\n  σ,ρ,β = p\n  x,y,z = u\n  du[1] = dx = σ*(y-x)\n  du[2] = dy = x*(ρ-z) - y\n  du[3] = dz = x*y - β*z\nend","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's get a solution of the system with parameter values σ=10.0 ρ=28.0 β=8/3 to use as our data. We define some convenience functions model_ode (to create an ODEProblem) and solve_model(to obtain solution of the ODEProblem) to use in a custom objective function later.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"u0 = [1.0;0.0;0.0]\nt = 0.0:0.01:1.0\ntspan = (0.0,1.0)\nmodel_ode(p_) = ODEProblem(g, u0, tspan,p_)\nsolve_model(mp_) = OrdinaryDiffEq.solve(model_ode(mp_), Tsit5(),saveat=0.01)\nmock_data = Array(solve_model([10.0,28.0,8/3]))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Now we define a custom objective function to pass for optimization to JuMP using the build_loss_objective described above provided by DiffEqParamEstim that defines an objective function for the parameter estimation problem.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"loss_objective(mp_, dat) = build_loss_objective(model_ode(mp_), Tsit5(), L2Loss(t,dat))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"We create a JuMP model, variables, set the objective function and the choice of optimization algorithm to be used in the JuMP syntax. You can read more about this in JuMP's documentation.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"juobj(args...) = loss_objective(args, mock_data)(args)\njumodel = Model()\nJuMP.register(jumodel, :juobj, 3, juobj, autodiff=true)\n@variables jumodel begin\n    σ,(start=8)\n    ρ,(start=25.0)\n    β,(start=10/3)\nend\n@NLobjective(jumodel, Min, juobj(σ, ρ, β))\nsetsolver(jumodel, NLoptSolver(algorithm=:LD_MMA))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's call the optimizer to obtain the fitted parameter values.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = JuMP.solve(jumodel)\nbest_mp = getvalue.(getindex.((jumodel,), Symbol.(jumodel.colNames)))","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's compare the solution at the obtained parameter values and our data.","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = OrdinaryDiffEq.solve(best_mp |> model_ode, Tsit5())\nplot(getindex.(sol.(t),1))\nscatter!(mock_data, markersize=2)","category":"page"},{"location":"tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"(Image: jumpestimationplot)","category":"page"},{"location":"tutorials/stochastic_evaluations/#Parameter-Estimation-for-Stochastic-Differential-Equations-and-Ensembles","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"","category":"section"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We can use any DEProblem, which not only includes DAEProblem and DDEProblems, but also stochastic problems. In this case, let's use the generalized maximum likelihood to fit the parameters of an SDE's ensemble evaluation.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Let's use the same Lotka-Volterra equation as before, but this time add noise:","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"pf_func = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0]\npg_func = function (du,u,p,t)\n  du[1] = 1e-6u[1]\n  du[2] = 1e-6u[2]\nend\nprob = SDEProblem(pf_func,pg_func,u0,tspan,p)\nsol = solve(prob,SRIW1())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now lets generate a dataset from 10,000 solutions of the SDE","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0, stop=10, length=200))\nfunction generate_data(t)\n  sol = solve(prob,SRIW1())\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(t) for i in 1:10000]))","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now let's estimate the parameters. Instead of using single runs from the SDE, we will use a EnsembleProblem. This means that it will solve the SDE N times to come up with an approximate probability distribution at each time point and use that in the likelihood estimate.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"monte_prob = EnsembleProblem(prob)","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We use Optim.jl for optimization below","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"obj = build_loss_objective(monte_prob,SOSRI(),L2Loss(t,aggregate_data),\n                                     maxiters=10000,verbose=false,num_monte = 1000,\n                                     parallel_type = :threads)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Parameter Estimation in case of SDE's with a regular L2Loss can have poor accuracy due to only fitting against the mean properties as mentioned in First Differencing.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Results of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [6.070728870478734,5.113357737345448]\n * Minimum: 1.700440e+03\n * Iterations: 14\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 1.00e-03\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.81e-07 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 2.34e+00\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 61\n * Gradient Calls: 61","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Instead when we use L2Loss with first differencing enabled we get much more accurate estimates.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":" obj = build_loss_objective(monte_prob,SRIW1(),L2Loss(t,data,differ_weight=1.0,data_weight=0.5),maxiters=1000,\n                                  verbose=false,verbose_opt=false,verbose_steps=1,num_monte=50)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())\nResults of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [1.5010687426045128,1.0023453619050238]\n * Minimum: 1.166650e-01\n * Iterations: 16\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 6.84e-09\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.85e-06 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 1.81e-01\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 118\n * Gradient Calls: 118","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Here, we see that we successfully recovered the drift parameter, and got close to the original noise parameter after searching a two-orders-of-magnitude range.","category":"page"},{"location":"methods/alternative_objectives/#Alternative-Objective-Functions","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"","category":"section"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"These are objective functions made to be used with special fitting packages.","category":"page"},{"location":"methods/alternative_objectives/#LeastSquaresOptim.jl-objective","page":"Alternative Objective Functions","title":"LeastSquaresOptim.jl objective","text":"","category":"section"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"build_lsoptim_objective builds an objective function to be used with LeastSquaresOptim.jl.","category":"page"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"build_lsoptim_objective(prob,tspan,t,data;\n                        prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p),\n                        kwargs...)","category":"page"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"The arguments are the same as build_loss_objective.","category":"page"},{"location":"methods/alternative_objectives/#lm_fit","page":"Alternative Objective Functions","title":"lm_fit","text":"","category":"section"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"lm_fit is a function for fitting the parameters of an ODE using the Levenberg-Marquardt algorithm. This algorithm is really bad and thus not recommended since, for example, the Optim.jl algorithms on an L2 loss are more performant and robust. However, this is provided for completeness as most other differential equation libraries use an LM-based algorithm, so this allows one to test the increased effectiveness of not using LM.","category":"page"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"lm_fit(prob::DEProblem,tspan,t,data,p0;\n       prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p),\n       kwargs...)","category":"page"},{"location":"methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"The arguments are similar to before, but with p0 being the initial conditions for the parameters and the kwargs as the args passed to the LsqFit curve_fit function (which is used for the LM solver). This returns the fitted parameters.","category":"page"},{"location":"methods/recommended_methods/#Recommended-Methods","page":"Recommended Methods","title":"Recommended Methods","text":"","category":"section"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"The recommended method is to use build_loss_objective with the optimizer of your choice. This method can thus be paired with global optimizers from packages like BlackBoxOptim.jl or NLopt.jl which can be much less prone to finding local minima than local optimization methods. Also, it allows the user to define the cost function in the way they choose as a function loss(sol), and thus can fit using any cost function on the solution, making it applicable to fitting non-temporal data and other types of problems. Also, build_loss_objective works for all of the DEProblem types, allowing it to optimize parameters on ODEs, SDEs, DDEs, DAEs, etc.","category":"page"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"However, this method requires repeated solution of the differential equation. If the data is temporal data, the most efficient method is the two_stage_method which does not require repeated solutions but is not as accurate. Usage of the two_stage_method should have a post-processing step which refines using a method like build_loss_objective.","category":"page"},{"location":"methods/optimization_based_methods/#Optimization-Based-Methods","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Objective-Function-Builders","page":"Optimization-Based Methods","title":"The Objective Function Builders","text":"","category":"section"},{"location":"methods/optimization_based_methods/#Standard-Nonlinear-Regression","page":"Optimization-Based Methods","title":"Standard Nonlinear Regression","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"build_loss_objective builds an objective function to be used with Optim.jl and MathProgBase-associated solvers like NLopt.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function build_loss_objective(prob::DEProblem,alg,loss_func\n                              regularization=nothing;\n                              mpg_autodiff = false,\n                              verbose_opt = false,\n                              verbose_steps = 100,\n                              prob_generator = (prob,p)->remake(prob,p=p),\n                              kwargs...)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The first argument is the DEProblem to solve, and next is the alg to use. The alg must match the problem type, which can be any DEProblem (ODEs, SDEs, DAEs, DDEs, etc.). regularization defaults to nothing which has no regularization function. One can also choose verbose_opt and verbose_steps, which, in the optimization routines, will print the steps and the values at the steps every verbose_steps steps. mpg_autodiff uses autodifferentiation to define the derivative for the MathProgBase solver. The extra keyword arguments are passed to the differential equation solver.","category":"page"},{"location":"methods/optimization_based_methods/#Multiple-Shooting","page":"Optimization-Based Methods","title":"Multiple Shooting","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Multiple Shooting is generally used in Boundary Value Problems (BVP) and is more robust than the regular objective function used in these problems. It proceeds as follows:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Divide up the time span into short time periods and solve the equation with the current parameters which here consist of both, the parameters of the differential equations and also the initial values for the short time periods.\nThis objective additionally involves a discontinuity error term that imposes higher cost if the end of the solution of one time period doesn't match the beginning of the next one.\nMerge the solutions from the shorter intervals and then calculate the loss.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function multiple_shooting_objective(prob::DiffEqBase.DEProblem,alg,loss,\n                              regularization=nothing;prior=nothing,\n                              mpg_autodiff = false,discontinuity_weight=1.0,\n                              verbose_opt = false,\n                              prob_generator = STANDARD_PROB_GENERATOR,\n                              autodiff_prototype = mpg_autodiff ? zeros(init_N_params) : nothing,\n                              autodiff_chunk = mpg_autodiff ? ForwardDiff.Chunk(autodiff_prototype) : nothing,\n                              kwargs...)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For consistency multiple_shooting_objective takes exactly the same arguments as build_loss_objective. It also has the option for discontinuity_error as a keyword argument which assigns weight to the error occurring due to the discontinuity that arises from the breaking up of the time span.","category":"page"},{"location":"methods/optimization_based_methods/#Detailed-Explanations-of-Arguments","page":"Optimization-Based Methods","title":"Detailed Explanations of Arguments","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Loss-Function","page":"Optimization-Based Methods","title":"The Loss Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"loss_func(sol)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"is a function which reduces the problem's solution to a scalar which the optimizer will try to minimize. While this is very flexible, two convenience routines are included for fitting to data with standard cost functions:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t,data;differ_weight=nothing,data_weight=nothing,\n              colloc_grad=nothing,dudt=nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"where t is the set of timepoints which the data is found at, and data are the values that are known where each column corresponds to measures of the values of the system. L2Loss is an optimized version of the L2-distance. The data_weight is a scalar or vector of weights for the loss function which must match the size of the data. Note that minimization of a weighted L2Loss is equivalent to maximum likelihood estimation of a heteroskedastic Normally distributed likelihood. differ_weight allows one to add a weight on the first differencing terms sol[i+1]-sol[i] against the data first differences. This smooths out the loss term and can make it easier to fit strong solutions of stochastic models, but is zero (nothing) by default. Additionally, colloc_grad allows one to give a matrix of the collocation gradients for the data. This is used to add an interpolation derivative term, like the two-stage method. A convenience function colloc_grad(t,data) returns a collocation gradient from a 3rd order spline calculated by Dierckx.jl, which can be used as the colloc_grad. Note that, with a collocation gradient and regularization, this loss is equivalent to a 4DVAR.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Additionally, we include a more flexible log-likelihood approach:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"LogLikeLoss(t,distributions,diff_distributions=nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"In this case, there are two forms. The simple case is where distributions[i,j] is the likelihood distributions from a UnivariateDistribution from Distributions.jl, where it corresponds to the likelihood at t[i] for component j. The second case is where distributions[i] is a MultivariateDistribution which corresponds to the likelihood at t[i] over the vector of components. This likelihood function then calculates the negative of the total loglikelihood over time as its objective value (negative since optimizers generally find minimums, and thus this corresponds to maximum likelihood estimation). The third term, diff_distributions, acts similarly but allows putting a distribution on the first difference terms sol[i+1]-sol[i].","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Note that these distributions can be generated via fit_mle on some dataset against some chosen distribution type.","category":"page"},{"location":"methods/optimization_based_methods/#Note-About-Loss-Functions","page":"Optimization-Based Methods","title":"Note About Loss Functions","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For parameter estimation problems, it's not uncommon for the optimizers to hit unstable regions of parameter space. This causes warnings that the solver exited early, and the built-in loss functions like L2Loss automatically handle this. However, if using a user-supplied loss function, you should make sure it's robust to these issues. One common pattern is to apply infinite loss when the integration is not successful. Using the retcodes, this can be done via:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function my_loss_function(sol)\n   tot_loss = 0.0\n   if any((s.retcode != :Success for s in sol))\n     tot_loss = Inf\n   else\n     # calculation for the loss here\n   end\n   tot_loss\nend","category":"page"},{"location":"methods/optimization_based_methods/#Note-on-First-Differencing","page":"Optimization-Based Methods","title":"Note on First Differencing","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t,data,differ_weight=0.3,data_weight=0.7)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"First differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the L2Loss alone leads to non-identifiable parameters but adding a first differencing term makes it more identifiable. This can be noted on stochastic differential equation models, where this aims to capture the autocorrelation and therefore helps us avoid getting the same stationary distribution despite different trajectories and thus wrong parameter estimates.","category":"page"},{"location":"methods/optimization_based_methods/#The-Regularization-Function","page":"Optimization-Based Methods","title":"The Regularization Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization can be any function of p, the parameter vector:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"regularization(p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The Regularization helper function builds a regularization using a penalty function penalty from PenaltyFunctions.jl:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Regularization(λ,penalty=L2Penalty())","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization defaults to L2 if no penalty function is specified. λ is the weight parameter for the addition of the regularization term.","category":"page"},{"location":"methods/optimization_based_methods/#The-Problem-Generator-Function","page":"Optimization-Based Methods","title":"The Problem Generator Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The argument prob_generator allows one to specify a function for generating new problems from a given parameter set. By default, this just builds a new problem which fixes the element types in a way that's autodifferentiation compatible and adds the new parameter vector p. For example, the code for this is:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Then the new problem with these new values is returned.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"One can use this to change the meaning of the parameters using this function. For example, if one instead wanted to optimize the initial conditions for a function without parameters, you could change this to:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob.f,u0=p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"which simply uses p as the initial condition in the initial value problem.","category":"page"},{"location":"methods/optimization_based_methods/#Using-the-Objectives-for-MAP-estimates","page":"Optimization-Based Methods","title":"Using the Objectives for MAP estimates","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"You can also add a prior option to build_loss_objective and multiple_shooting_objective that essentially turns it into MAP by multiplying the loglikelihood (the cost) by the prior. The option was added as a keyword argument priors, it can take in either an array of univariate distributions for each of the parameters or a multivariate distribution.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"ms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);priors=priors,discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)","category":"page"},{"location":"#DiffEqParamEstim.jl:-Parameter-Estimation-for-Differential-Equations","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"DiffEqParamEstim.jl is a package for simplified parameter estimation with  DifferentialEquations.jl While not as expansive as SciMLSensitivity.jl,  it's a simple and helpful for new users who want to quickly run standard parameter  estimation routines for model callibration on not too large of models (<100 parameters  or ODEs). ","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"note: Note\nFor much larger models and more complex setups (multiple datasets, batching, etc.) see  SciMLSensitivity.","category":"page"},{"location":"#Installation","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"To install DiffEqParamEstim.jl, use the Julia package manager:","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg\nPkg.add(\"DiffEqParamEstim\")","category":"page"},{"location":"#Contributing","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Contributing","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"\">project</a> file.","category":"page"},{"location":"tutorials/ODE_inference/#Optimization-Based-ODE-Parameter-Estimation","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"","category":"section"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We choose to optimize the parameters on the Lotka-Volterra equation. We do so by defining the function as a function with parameters:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - u[1]*u[2]\n  du[2] = dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We create data using the numerical result with a=1.5:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"sol = solve(prob,Tsit5())\nt = collect(range(0,stop=10,length=200))\nusing RecursiveArrayTools # for VectorOfArray\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we used VectorOfArray from RecursiveArrayTools.jl to turn the result of an ODE into a matrix.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"If we plot the solution with the parameter at a=1.42, we get the following:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Not Fit)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Notice that after one period this solution begins to drift very far off: this problem is sensitive to the choice of a.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"To build the objective function for Optim.jl, we simply call the build_loss_objective function:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set maxiters in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set verbose=false because this divergence can get noisy.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Before optimizing, let's visualize our cost function by plotting it for a range of parameter values:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"vals = 0.0:0.1:10.0\nusing Plots; plotly()\nplot(vals,[cost_function(i) for i in vals],yscale=:log10,\n     xaxis = \"Parameter\", yaxis = \"Cost\", title = \"1-Parameter Cost Function\",\n     lw = 3)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Now this cost function can be used with Optim.jl in order to get the parameters. For example, we can use Brent's algorithm to search for the best solution on the interval [0,10] by:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using Optim\nresult = optimize(cost_function, 0.0, 10.0)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This returns result.minimizer[1]==1.5 as the best parameter to match the data. When we plot the fitted equation on the data, we receive the following:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Fit)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Thus we see that after fitting, the lines match up with the generated data and receive the right parameter value.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use the multivariate optimization functions. For example, we can use the BFGS algorithm to optimize the parameter starting at a=1.42 using:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"result = optimize(cost_function, [1.42], BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Note that some of the algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the documentation for Optim.jl. We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus following the Optim.jl documentation we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"lower = [0.0]\nupper = [3.0]\nresult = optimize(cost_function, lower, upper, [1.42], Fminbox(BFGS()))","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let's use the Lotka-Volterra equation with all parameters free:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function f2(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f2,u0,tspan,p)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can build an objective function and solve the multiple parameter version just as before:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),\n                                      maxiters=10000,verbose=false)\nresult_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use First-Differences in L2Loss by passing the kwarg differ_weight which decides the contribution of the differencing loss to the total loss.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data,differ_weight=0.3,data_weight=0.7),\n                                      maxiters=10000,verbose=false)\nresult_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"To solve it using LeastSquaresOptim.jl, we use the build_lsoptim_objective function:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_lsoptim_objective(prob1,t,data,Tsit5())","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The result is a cost function which can be used with LeastSquaresOptim. For more details, consult the documentation for LeastSquaresOptim.jl:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using LeastSquaresOptim # for LeastSquaresProblem\nx = [1.3,0.8,2.8,1.2]\nres = optimize!(LeastSquaresProblem(x = x, f! = cost_function,\n                output_length = length(t)*length(prob.u0)),\n                LeastSquaresOptim.Dogleg(LeastSquaresOptim.LSMR()))","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can see the results are:","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"println(res.minimizer)\n\nResults of Optimization Algorithm\n * Algorithm: Dogleg\n * Minimizer: [1.4995074428834114,0.9996531871795851,3.001556360700904,1.0006272074128821]\n * Sum of squares at Minimum: 0.035730\n * Iterations: 63\n * Convergence: true\n * |x - x'| < 1.0e-15: true\n * |f(x) - f(x')| / |f(x)| < 1.0e-14: false\n * |g(x)| < 1.0e-14: false\n * Function Calls: 64\n * Gradient Calls: 9\n * Multiplication Calls: 135","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"and thus this algorithm was able to correctly identify all four parameters.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use Multiple Shooting method by creating a multiple_shooting_objective","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function ms_f(du,u,p,t)\n  dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\nms_u0 = [1.0;1.0]\ntspan = (0.0,10.0)\nms_p = [1.5,1.0]\nms_prob = ODEProblem(ms_f,ms_u0,tspan,ms_p)\nt = collect(range(0,stop=10,length=200))\ndata = Array(solve(ms_prob,Tsit5(),saveat=t,abstol=1e-12,reltol=1e-12))\nbound = Tuple{Float64, Float64}[(0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10)]\n\n\nms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This creates the objective function that can be passed to an optimizer from which we can then get the parameter values and the initial values of the short time periods keeping in mind the indexing.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"# ]add BlackBoxOptim\nusing BlackBoxOptim\n\nresult = bboptimize(ms_obj;SearchRange = bound, MaxSteps = 21e3)\nresult.archive_output.best_candidate[end-1:end]","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Giving us the results as","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Starting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},BlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound{BlackBoxOptim.RangePerDimSearchSpace}}\n\nOptimization stopped after 21001 steps and 136.60030698776245 seconds\nTermination reason: Max number of steps (21000) reached\nSteps per second = 153.7405036862868\nFunction evals per second = 154.43596332393247\nImprovements/step = 0.17552380952380953\nTotal function evaluations = 21096\n\n\nBest candidate found: [0.997396, 1.04664, 3.77834, 0.275823, 2.14966, 4.33106, 1.43777, 0.468442, 6.22221, 0.673358, 0.970036, 2.05182, 2.4216, 0.274394, 5.64131, 3.38132, 1.52826, 1.01721]\n\nFitness: 0.126884213\n\nOut[4]:2-element Array{Float64,1}:\n        1.52826\n        1.01721","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here as our model had 2 parameters, we look at the last two indexes of result to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section.","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The objective function for Two Stage method can be created and passed to an optimizer as","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"two_stage_obj = two_stage_method(ms_prob,t,data)\nresult = Optim.optimize(two_stage_obj, [1.3,0.8,2.8,1.2], Optim.BFGS()\n)\nResults of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.3,0.8,2.8,1.2]\n * Minimizer: [1.5035938533664717,0.9925731153746833, ...]\n * Minimum: 1.513400e+00\n * Iterations: 9\n * Convergence: true\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 4.58e-10\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.87e-16 |f(x)|\n   * |g(x)| ≤ 1.0e-08: true\n     |g(x)| = 7.32e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 31\n * Gradient Calls: 31","category":"page"},{"location":"tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The default kernel used in the method is Epanechnikov others that are available are Uniform,  Triangular, Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic and Sigmoid, this can be passed by the kernel keyword argument. loss_func keyword argument can be used to pass the loss function (cost function) you want  to use and mpg_autodiff enables Auto Differentiation.","category":"page"}]
}
