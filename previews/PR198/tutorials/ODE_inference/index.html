<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimization-Based ODE Parameter Estimation · DiffEqParamEstim.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://docs.sciml.ai/DiffEqParamEstim/stable/tutorials/ODE_inference/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqParamEstim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DiffEqParamEstim.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqParamEstim.jl: Parameter Estimation for Differential Equations</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Optimization-Based ODE Parameter Estimation</a></li><li><a class="tocitem" href="../global_optimization/">Global Optimization via NLopt</a></li><li><a class="tocitem" href="../jump/">Using JuMP with DiffEqParamEstim</a></li><li><a class="tocitem" href="../generalized_likelihood/">Generalized Likelihood Inference</a></li><li><a class="tocitem" href="../stochastic_evaluations/">Parameter Estimation for Stochastic Differential Equations and Ensembles</a></li><li><a class="tocitem" href="../ensemble/">Fitting Ensembles of ODE Models to Data</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../../methods/recommended_methods/">Recommended Methods</a></li><li><a class="tocitem" href="../../methods/optimization_based_methods/">Optimization-Based Methods</a></li><li><a class="tocitem" href="../../methods/collocation_loss/">Two Stage method (Non-Parametric Collocation)</a></li><li><a class="tocitem" href="../../methods/alternative_objectives/">Alternative Objective Functions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Optimization-Based ODE Parameter Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimization-Based ODE Parameter Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqParamEstim.jl/blob/master/docs/src/tutorials/ODE_inference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimization-Based-ODE-Parameter-Estimation"><a class="docs-heading-anchor" href="#Optimization-Based-ODE-Parameter-Estimation">Optimization-Based ODE Parameter Estimation</a><a id="Optimization-Based-ODE-Parameter-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Based-ODE-Parameter-Estimation" title="Permalink"></a></h1><p>We choose to optimize the parameters on the Lotka-Volterra equation. We do so by defining the function as a function with parameters:</p><pre><code class="language-julia hljs">using DifferentialEquations, RecursiveArrayTools, Plots, Optim, DiffEqParamEstim

function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - u[1]*u[2]
  du[2] = dy = -3*u[2] + u[1]*u[2]
end

u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5]
prob = ODEProblem(f,u0,tspan,p)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ODEProblem with uType Vector{Float64} and tType Float64. In-place: true
timespan: (0.0, 10.0)
u0: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>We create data using the numerical result with <code>a=1.5</code>:</p><pre><code class="language-julia hljs">sol = solve(prob,Tsit5())
t = collect(range(0,stop=10,length=200))
using RecursiveArrayTools # for VectorOfArray
randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])
data = convert(Array,randomized)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×200 Matrix{Float64}:
 0.99287   1.03039   1.07495   1.11013   …  0.988852  1.00955   1.05027
 0.994569  0.899529  0.814101  0.759921     1.09336   0.995377  0.899043</code></pre><p>Here we used <code>VectorOfArray</code> from <a href="https://docs.sciml.ai/RecursiveArrayTools/stable/">RecursiveArrayTools.jl</a> to turn the result of an ODE into a matrix.</p><p>If we plot the solution with the parameter at <code>a=1.42</code>, we get the following:</p><p><img src="../../assets/paramest_notfit.png" alt="Parameter Estimation Not Fit"/></p><p>Notice that after one period this solution begins to drift very far off: this problem is sensitive to the choice of <code>a</code>.</p><p>To build the objective function for Optim.jl, we simply call the <code>build_loss_objective</code> function:</p><pre><code class="language-julia hljs">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),
                                     maxiters=10000,verbose=false)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::SciMLBase.OptimizationFunction{true, SciMLBase.NoAD, DiffEqParamEstim.var&quot;#29#30&quot;{Nothing, typeof(DiffEqParamEstim.STANDARD_PROB_GENERATOR), Base.Pairs{Symbol, Integer, Tuple{Symbol, Symbol}, NamedTuple{(:maxiters, :verbose), Tuple{Int64, Bool}}}, SciMLBase.ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, SciMLBase.ODEFunction{true, SciMLBase.AutoSpecialize, typeof(Main.f), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}, SciMLBase.StandardODEProblem}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, L2Loss{Vector{Float64}, Matrix{Float64}, Nothing, Nothing, Nothing}, Nothing, Tuple{}}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}) (generic function with 1 method)</code></pre><p>This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set <code>maxiters</code> in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set <code>verbose=false</code> because this divergence can get noisy.</p><p>Before optimizing, let&#39;s visualize our cost function by plotting it for a range of parameter values: yscale</p><pre><code class="language- hljs">vals = 0.0:0.1:10.0
plot(vals,[cost_function(i) for i in vals],yscale=:log10,
     xaxis = &quot;Parameter&quot;, yaxis = &quot;Cost&quot;, title = &quot;1-Parameter Cost Function&quot;,
     lw = 3)</code></pre><p>Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).</p><p>Now this cost function can be used with Optim.jl in order to get the parameters. For example, we can use Brent&#39;s algorithm to search for the best solution on the interval <code>[0,10]</code> by:</p><pre><code class="language- hljs">using Optim
result = optimize(cost_function, 0.0, 10.0)</code></pre><p>This returns <code>result.minimizer[1]==1.5</code> as the best parameter to match the data. When we plot the fitted equation on the data, we receive the following:</p><p><img src="../../assets/paramest_fit.png" alt="Parameter Estimation Fit"/></p><p>Thus we see that after fitting, the lines match up with the generated data and receive the right parameter value.</p><p>We can also use the multivariate optimization functions. For example, we can use the <code>BFGS</code> algorithm to optimize the parameter starting at <code>a=1.42</code> using:</p><pre><code class="language- hljs">result = optimize(cost_function, [1.42], BFGS())</code></pre><p>Note that some of the algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the <a href="https://julianlsolvers.github.io/Optim.jl/stable/">documentation for Optim.jl</a>. We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus <a href="https://julianlsolvers.github.io/Optim.jl/stable/#user/minimization/#box-constrained-optimization">following the Optim.jl documentation</a> we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm:</p><pre><code class="language- hljs">lower = [0.0]
upper = [3.0]
result = optimize(cost_function, lower, upper, [1.42], Fminbox(BFGS()))</code></pre><p>Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let&#39;s use the Lotka-Volterra equation with all parameters free:</p><pre><code class="language-julia hljs">function f2(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end

u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(f2,u0,tspan,p)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ODEProblem with uType Vector{Float64} and tType Float64. In-place: true
timespan: (0.0, 10.0)
u0: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>We can build an objective function and solve the multiple parameter version just as before:</p><pre><code class="language- hljs">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),
                                      maxiters=10000,verbose=false)
result_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())</code></pre><p>We can also use First-Differences in L2Loss by passing the kwarg <code>differ_weight</code> which decides the contribution of the differencing loss to the total loss.</p><pre><code class="language- hljs">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data,differ_weight=0.3,data_weight=0.7),
                                      maxiters=10000,verbose=false)
result_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())</code></pre><p>To solve it using LeastSquaresOptim.jl, we use the <code>build_lsoptim_objective</code> function:</p><pre><code class="language- hljs">cost_function = build_lsoptim_objective(prob,t,data,Tsit5())</code></pre><p>The result is a cost function which can be used with LeastSquaresOptim. For more details, consult the <a href="https://github.com/matthieugomez/LeastSquaresOptim.jl">documentation for LeastSquaresOptim.jl</a>:</p><pre><code class="language-julia hljs">using LeastSquaresOptim # for LeastSquaresProblem
x = [1.3,0.8,2.8,1.2]
res = optimize!(LeastSquaresProblem(x = x, f! = cost_function,
                output_length = length(t)*length(prob.u0)),
                LeastSquaresOptim.Dogleg(LeastSquaresOptim.LSMR()))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Results of Optimization Algorithm
 * Algorithm: Dogleg
 * Minimizer: [NaN,NaN,NaN,NaN]
 * Sum of squares at Minimum: 0.000000
 * Iterations: 1
 * Convergence: true
 * |x - x&#39;| &lt; 1.0e-08: false
 * |f(x) - f(x&#39;)| / |f(x)| &lt; 1.0e-08: true
 * |g(x)| &lt; 1.0e-08: false
 * Function Calls: 2
 * Gradient Calls: 1
 * Multiplication Calls: 803
</code></pre><p>and thus this algorithm was able to correctly identify all four parameters.</p><p>We can also use Multiple Shooting method by creating a <code>multiple_shooting_objective</code></p><pre><code class="language-julia hljs">function ms_f(du,u,p,t)
  dx = p[1]*u[1] - p[2]*u[1]*u[2]
  dy = -3*u[2] + u[1]*u[2]
end
ms_u0 = [1.0;1.0]
tspan = (0.0,10.0)
ms_p = [1.5,1.0]
ms_prob = ODEProblem(ms_f,ms_u0,tspan,ms_p)
t = collect(range(0,stop=10,length=200))
data = Array(solve(ms_prob,Tsit5(),saveat=t,abstol=1e-12,reltol=1e-12))
bound = Tuple{Float64, Float64}[(0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10)]


ms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::SciMLBase.OptimizationFunction{true, SciMLBase.NoAD, DiffEqParamEstim.var&quot;#43#48&quot;{Nothing, Float64, DiffEqParamEstim.var&quot;#1#2&quot;, Base.Pairs{Symbol, Float64, Tuple{Symbol, Symbol}, NamedTuple{(:abstol, :reltol), Tuple{Float64, Float64}}}, SciMLBase.ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, SciMLBase.ODEFunction{true, SciMLBase.AutoSpecialize, typeof(Main.ms_f), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}, SciMLBase.StandardODEProblem}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, L2Loss{Vector{Float64}, Matrix{Float64}, Nothing, Nothing, Nothing}, Nothing}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}) (generic function with 1 method)</code></pre><p>This creates the objective function that can be passed to an optimizer from which we can then get the parameter values and the initial values of the short time periods keeping in mind the indexing.</p><pre><code class="language-julia hljs"># ]add BlackBoxOptim
using BlackBoxOptim

result = bboptimize(ms_obj;SearchRange = bound, MaxSteps = 21e3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BlackBoxOptim.OptimizationResults(&quot;adaptive_de_rand_1_bin_radiuslimited&quot;, &quot;Max number of steps (21000) reached&quot;, 21001, 1.671525362444286e9, 5.150054931640625, BlackBoxOptim.ParamsDictChain[BlackBoxOptim.ParamsDictChain[Dict{Symbol, Any}(:RngSeed =&gt; 765991, :SearchRange =&gt; [(0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0), (0.0, 10.0)], :MaxSteps =&gt; 21000),Dict{Symbol, Any}()],Dict{Symbol, Any}(:CallbackInterval =&gt; -1.0, :TargetFitness =&gt; nothing, :TraceMode =&gt; :compact, :FitnessScheme =&gt; BlackBoxOptim.ScalarFitnessScheme{true}(), :MinDeltaFitnessTolerance =&gt; 1.0e-50, :NumDimensions =&gt; :NotSpecified, :FitnessTolerance =&gt; 1.0e-8, :TraceInterval =&gt; 0.5, :MaxStepsWithoutProgress =&gt; 10000, :MaxSteps =&gt; 10000…)], 21097, BlackBoxOptim.ScalarFitnessScheme{true}(), BlackBoxOptim.TopListArchiveOutput{Float64, Vector{Float64}}(3.7963240954035945e-6, [0.9999877833050822, 0.9999572623257812, 1.0000295738167666, 0.99989731630132, 1.0000087782959568, 0.999959431501904, 0.9998539640429432, 0.9999251307206347, 1.0000086883043786, 1.0001627009728287, 0.9999082572827672, 1.0001458701166843, 1.0000997163979903, 1.0000591329622854, 0.9998333861707335, 0.9999198176058877, 3.2738680534368982, 3.475498530875719]), BlackBoxOptim.PopulationOptimizerOutput{BlackBoxOptim.FitPopulation{Float64}}(BlackBoxOptim.FitPopulation{Float64}([1.000138775468645 1.0000339138899057 … 0.9999801512313007 0.9997167876423292; 1.0000079133705988 1.000028386233395 … 0.999993235175935 1.0000973019985806; … ; 3.987186276296794 2.8768016208370537 … 2.6787362226297295 6.576690805675898; 6.4843267198899985 5.401309688168052 … 8.747208707610948 8.55451044172085], NaN, [6.168399688375431e-6, 6.348129889993948e-6, 5.040624535023441e-6, 4.662220691947281e-6, 4.7084154571474466e-6, 4.1693547534386546e-6, 4.877734632563194e-6, 6.130915526900144e-6, 4.096215157318782e-6, 5.563623148059021e-6  …  6.77113122049219e-6, 4.099634131586008e-6, 7.460656588184127e-6, 6.3721721581578545e-6, 6.7832840755759585e-6, 6.36722805725089e-6, 9.433845530338493e-6, 6.969214786501454e-6, 3.914017858107369e-6, 6.479855887953221e-6], 0, BlackBoxOptim.Candidate{Float64}[BlackBoxOptim.Candidate{Float64}([1.000210636432196, 1.0001642074343562, 0.9996649809840138, 1.0001803338930266, 1.0001304138071123, 0.9998246426738451, 0.9998977845165239, 0.9999526115383232, 0.9999391621492727, 0.9998914217151702, 0.9997985840829495, 1.0001950860529343, 0.9998504380789331, 0.9999651843381653, 0.9999550553121258, 0.9999710622741814, 0.04917262357414548, 5.486800920567103], 28, 1.0428780227941073e-5, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.5740759761332956, 0.5115859786565731, 0.686841483799337, 0.2276216741164796, 0.658515447531129, 1.0, 1.0, 0.790373086241543, 1.0, 0.6344496178578349  …  0.6888351451334705, 1.0, 0.20498429684309977, 0.5974996053422823, 1.0, 0.6446713303774552, 0.7681540102872405, 0.5546311377079733, 0.9697713310358477, 0.39561942327091043], [0.3617340997208871, 0.8931940548979902, 0.2630259942456027, 0.15414563843716955, 0.9704334788095096, 0.9703111770294868, 0.7996231513060496, 0.2473803863860905, 0.28814393153915085, 1.0  …  0.17110494722182934, 0.04257581593302082, 0.08619741283080289, 0.12105092054367225, 0.9758088163074987, 0.12029832941656032, 1.0, 0.005677075366236711, 0.1415873364103841, 0.15524303500184897])), 0), BlackBoxOptim.Candidate{Float64}([0.9997400534072285, 1.00006748840153, 1.000615399214936, 0.9999663931887484, 1.0001203024126102, 1.000538700418266, 1.000088834966499, 1.0000097011998401, 0.9996912351441323, 0.9998914217151702, 1.0002277290229487, 0.9997085469303146, 1.000503266311344, 1.0003803125341328, 0.9999879346798054, 0.9997595742278, 4.398997756591557, 2.47671809464701], 28, 3.850548555741879e-5, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.5740759761332956, 0.5115859786565731, 0.686841483799337, 0.2276216741164796, 0.658515447531129, 1.0, 1.0, 0.790373086241543, 1.0, 0.6344496178578349  …  0.6888351451334705, 1.0, 0.20498429684309977, 0.5974996053422823, 1.0, 0.6446713303774552, 0.7681540102872405, 0.5546311377079733, 0.9697713310358477, 0.39561942327091043], [0.3617340997208871, 0.8931940548979902, 0.2630259942456027, 0.15414563843716955, 0.9704334788095096, 0.9703111770294868, 0.7996231513060496, 0.2473803863860905, 0.28814393153915085, 1.0  …  0.17110494722182934, 0.04257581593302082, 0.08619741283080289, 0.12105092054367225, 0.9758088163074987, 0.12029832941656032, 1.0, 0.005677075366236711, 0.1415873364103841, 0.15524303500184897])), 0)], Base.Threads.SpinLock(0))))</code></pre><pre><code class="language-julia hljs">result.archive_output.best_candidate[end-1:end]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 3.2738680534368982
 3.475498530875719</code></pre><p>Here as our model had 2 parameters, we look at the last two indexes of <code>result</code> to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section.</p><p>The objective function for Two Stage method can be created and passed to an optimizer as</p><pre><code class="language- hljs">two_stage_obj = two_stage_method(ms_prob,t,data)
result = Optim.optimize(two_stage_obj, [1.3,0.8,2.8,1.2], Optim.BFGS())</code></pre><p>The default kernel used in the method is <code>Epanechnikov</code> others that are available are <code>Uniform</code>,  <code>Triangular</code>, <code>Quartic</code>, <code>Triweight</code>, <code>Tricube</code>, <code>Gaussian</code>, <code>Cosine</code>, <code>Logistic</code> and <code>Sigmoid</code>, this can be passed by the <code>kernel</code> keyword argument. <code>loss_func</code> keyword argument can be used to pass the loss function (cost function) you want  to use and <code>mpg_autodiff</code> enables Auto Differentiation.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« DiffEqParamEstim.jl: Parameter Estimation for Differential Equations</a><a class="docs-footer-nextpage" href="../global_optimization/">Global Optimization via NLopt »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 20 December 2022 08:36">Tuesday 20 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
